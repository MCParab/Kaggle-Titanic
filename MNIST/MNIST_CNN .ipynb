{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import math \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in training data from train.csv \n",
    "dftrain = pd.read_csv('train.csv')\n",
    "dftrainFeatureVector = dftrain.drop(['label'], axis=1)\n",
    "trainFeatureVector = dftrainFeatureVector.values.astype(dtype = np.float32)\n",
    "trainFeatureVectorConvoFormat = trainFeatureVector.reshape(42000, 28, 28, 1)\n",
    "\n",
    "trainLabelsList = dftrain['label'].tolist()\n",
    "ohtrainLabelTensor = tf.one_hot(trainLabelsList, depth=10)\n",
    "ohtrainLabelNdarray = tf.Session().run(ohtrainLabelTensor).astype(dtype = np.float64)\n",
    "\n",
    "# Read in testing data from test.csv \n",
    "dftest = pd.read_csv('test.csv')\n",
    "testFeatureVector = dftest.values.astype(dtype = np.float32)\n",
    "testFeatureVectorConvoFormat = testFeatureVector.reshape(28000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBThB4Ii/EBQ/wTOk43qYyOZtAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28f0768898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying from csv data \n",
    "pixels = testFeatureVectorConvoFormat[0].reshape((28,28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct tensorflow graph \n",
    "\n",
    "# Define tensorflow graph \n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "K = 6 \n",
    "L = 12\n",
    "M = 24\n",
    "N = 200 \n",
    "\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))\n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))\n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "\n",
    "# The model\n",
    "stride = 1  # output is 28x28\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2  # output is 14x14\n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2  # output is 7x7\n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "# reshape the output from the third convolution for the fully connected layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "YY4 = tf.nn.dropout(Y4, pkeep)\n",
    "Ylogits = tf.matmul(YY4, W5) + B5\n",
    "Y = tf.nn.softmax(Ylogits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100\n",
    "\n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "predictions = tf.argmax(Y, 1)\n",
    "\n",
    "# training step, the learning rate is a placeholder\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: training accuracy:0.08 training loss: 10016.8 (lr:0.003)\n",
      "0: ********* test accuracy:0.0741 test loss: 10146.0\n",
      "100: training accuracy:0.93 training loss: 31.4235 (lr:0.0028585653310520707)\n",
      "200: training accuracy:0.93 training loss: 26.0527 (lr:0.0027240285123042826)\n",
      "300: training accuracy:0.93 training loss: 22.1612 (lr:0.0025960531316326675)\n",
      "400: training accuracy:0.91 training loss: 29.335 (lr:0.0024743191839261473)\n",
      "500: training accuracy:0.95 training loss: 15.3479 (lr:0.002358522270907074)\n",
      "500: ********* test accuracy:0.9705 test loss: 9.13874\n",
      "600: training accuracy:1.0 training loss: 3.09899 (lr:0.002248372839976982)\n",
      "700: training accuracy:0.94 training loss: 23.7815 (lr:0.002143595460184269)\n",
      "800: training accuracy:0.97 training loss: 5.8856 (lr:0.002043928133503354)\n",
      "900: training accuracy:0.98 training loss: 5.86822 (lr:0.001949121639703143)\n",
      "1000: training accuracy:1.0 training loss: 1.78846 (lr:0.0018589389131666372)\n",
      "1000: ********* test accuracy:0.9853 test loss: 4.57442\n",
      "1100: training accuracy:0.98 training loss: 4.02253 (lr:0.0017731544501034114)\n",
      "1200: training accuracy:0.99 training loss: 8.24808 (lr:0.0016915537446726768)\n",
      "1300: training accuracy:0.98 training loss: 4.06241 (lr:0.0016139327526069466)\n",
      "1400: training accuracy:0.98 training loss: 8.98094 (lr:0.0015400973809950877)\n",
      "1500: training accuracy:0.99 training loss: 1.53965 (lr:0.0014698630029489428)\n",
      "1500: ********* test accuracy:0.9876 test loss: 3.56112\n",
      "1600: training accuracy:0.99 training loss: 2.8201 (lr:0.0014030539959399427)\n",
      "1700: training accuracy:1.0 training loss: 0.894116 (lr:0.0013395033026513076)\n",
      "1800: training accuracy:1.0 training loss: 0.288238 (lr:0.0012790520132477375)\n",
      "1900: training accuracy:1.0 training loss: 0.925594 (lr:0.0012215489680180538)\n",
      "2000: training accuracy:0.98 training loss: 2.72497 (lr:0.0011668503793971828)\n",
      "2000: ********* test accuracy:0.9933 test loss: 2.28082\n",
      "2100: training accuracy:0.99 training loss: 2.51752 (lr:0.0011148194724223506)\n",
      "2200: training accuracy:1.0 training loss: 0.546631 (lr:0.0010653261427244307)\n",
      "2300: training accuracy:1.0 training loss: 1.04467 (lr:0.0010182466311992545)\n",
      "2400: training accuracy:1.0 training loss: 0.188105 (lr:0.0009734632145453863)\n",
      "2500: training accuracy:0.98 training loss: 2.32335 (lr:0.0009308639108945514)\n",
      "2500: ********* test accuracy:0.9973 test loss: 0.770422\n",
      "2600: training accuracy:0.99 training loss: 1.00302 (lr:0.0008903421997986366)\n",
      "2700: training accuracy:1.0 training loss: 0.0480773 (lr:0.0008517967558730855)\n",
      "2800: training accuracy:1.0 training loss: 0.730628 (lr:0.0008151311954306589)\n",
      "2900: training accuracy:1.0 training loss: 0.0764489 (lr:0.0007802538354720133)\n",
      "3000: training accuracy:1.0 training loss: 0.0573902 (lr:0.0007470774644304465)\n",
      "3000: ********* test accuracy:0.9974 test loss: 0.785471\n",
      "3100: training accuracy:1.0 training loss: 0.261147 (lr:0.0007155191240975549)\n",
      "3200: training accuracy:1.0 training loss: 0.260351 (lr:0.0006854999021845007)\n",
      "3300: training accuracy:0.99 training loss: 3.50162 (lr:0.000656944735000187)\n",
      "3400: training accuracy:1.0 training loss: 0.387193 (lr:0.0006297822197529307)\n",
      "3500: training accuracy:0.99 training loss: 2.94937 (lr:0.000603944436006291)\n",
      "3500: ********* test accuracy:0.9964 test loss: 0.867125\n",
      "3600: training accuracy:0.99 training loss: 2.14148 (lr:0.000579366775842601)\n",
      "3700: training accuracy:0.99 training loss: 1.85302 (lr:0.0005559877823095201)\n",
      "3800: training accuracy:1.0 training loss: 0.238938 (lr:0.0005337489957456418)\n",
      "3900: training accuracy:1.0 training loss: 0.0834448 (lr:0.0005125948076008895)\n",
      "4000: training accuracy:1.0 training loss: 0.305617 (lr:0.0004924723213861769)\n",
      "4000: ********* test accuracy:0.9993 test loss: 0.292071\n",
      "4100: training accuracy:1.0 training loss: 0.313295 (lr:0.0004733312204046323)\n",
      "4200: training accuracy:1.0 training loss: 0.0801313 (lr:0.00045512364193364755)\n",
      "4300: training accuracy:1.0 training loss: 0.130214 (lr:0.00043780405754314123)\n",
      "4400: training accuracy:1.0 training loss: 0.292529 (lr:0.00042132915925076824)\n",
      "4500: training accuracy:1.0 training loss: 0.0977291 (lr:0.00040565775122940656)\n",
      "4500: ********* test accuracy:0.9991 test loss: 0.264771\n",
      "4600: training accuracy:1.0 training loss: 0.568813 (lr:0.0003907506467961309)\n",
      "4700: training accuracy:1.0 training loss: 0.0460478 (lr:0.0003765705704250939)\n",
      "4800: training accuracy:1.0 training loss: 0.00226921 (lr:0.0003630820645392963)\n",
      "4900: training accuracy:1.0 training loss: 0.138713 (lr:0.00035025140084817447)\n",
      "5000: training accuracy:1.0 training loss: 0.0248574 (lr:0.00033804649600930654)\n",
      "5000: ********* test accuracy:0.9998 test loss: 0.114781\n",
      "5100: training accuracy:1.0 training loss: 0.024508 (lr:0.0003264368314033442)\n",
      "5200: training accuracy:1.0 training loss: 0.0199803 (lr:0.0003153933768215683)\n",
      "5300: training accuracy:1.0 training loss: 0.0242881 (lr:0.00030488851787524585)\n",
      "5400: training accuracy:0.99 training loss: 1.13659 (lr:0.0002948959869452743)\n",
      "5500: training accuracy:1.0 training loss: 0.0180389 (lr:0.00028539079749945195)\n",
      "5500: ********* test accuracy:1.0 test loss: 0.0673126\n",
      "5600: training accuracy:1.0 training loss: 0.0970614 (lr:0.00027634918161313215)\n",
      "5700: training accuracy:1.0 training loss: 0.0309376 (lr:0.0002677485305370315)\n",
      "5800: training accuracy:1.0 training loss: 0.155925 (lr:0.000259567338163581)\n",
      "5900: training accuracy:1.0 training loss: 0.0223586 (lr:0.00025178514725045394)\n",
      "6000: training accuracy:1.0 training loss: 0.0972316 (lr:0.00024438249826680544)\n",
      "6000: ********* test accuracy:0.9997 test loss: 0.100764\n",
      "6100: training accuracy:1.0 training loss: 0.0894027 (lr:0.00023734088073430873)\n",
      "6200: training accuracy:1.0 training loss: 0.114313 (lr:0.00023064268694131766)\n",
      "6300: training accuracy:1.0 training loss: 0.174087 (lr:0.00022427116791441654)\n",
      "6400: training accuracy:1.0 training loss: 0.0545525 (lr:0.00021821039153726202)\n",
      "6500: training accuracy:1.0 training loss: 0.0100701 (lr:0.00021244520271199385)\n",
      "6500: ********* test accuracy:1.0 test loss: 0.0654089\n",
      "6600: training accuracy:1.0 training loss: 0.494466 (lr:0.00020696118546359606)\n",
      "6700: training accuracy:1.0 training loss: 0.0741251 (lr:0.0002017446268924506)\n",
      "6800: training accuracy:1.0 training loss: 0.0104871 (lr:0.00019678248288494563)\n",
      "6900: training accuracy:1.0 training loss: 0.00214012 (lr:0.00019206234549639704)\n",
      "7000: training accuracy:1.0 training loss: 0.046345 (lr:0.00018757241192472366)\n",
      "7000: ********* test accuracy:1.0 test loss: 0.033066\n",
      "7100: training accuracy:1.0 training loss: 0.0201894 (lr:0.00018330145499729437)\n",
      "7200: training accuracy:1.0 training loss: 0.000573143 (lr:0.00017923879509714843)\n",
      "7300: training accuracy:1.0 training loss: 0.00572546 (lr:0.0001753742734583905)\n",
      "7400: training accuracy:1.0 training loss: 0.00129873 (lr:0.00017169822676398424)\n",
      "7500: training accuracy:1.0 training loss: 0.0170638 (lr:0.00016820146298242642)\n",
      "7500: ********* test accuracy:1.0 test loss: 0.0215809\n",
      "7600: training accuracy:1.0 training loss: 0.00242496 (lr:0.00016487523838288026)\n",
      "7700: training accuracy:1.0 training loss: 0.0272442 (lr:0.0001617112356712938)\n",
      "7800: training accuracy:1.0 training loss: 0.0073392 (lr:0.00015870154319283275)\n",
      "7900: training accuracy:1.0 training loss: 0.0483764 (lr:0.00015583863514862209)\n",
      "8000: training accuracy:1.0 training loss: 0.00179918 (lr:0.00015311535277732913)\n",
      "8000: ********* test accuracy:1.0 test loss: 0.0226432\n",
      "8100: training accuracy:1.0 training loss: 0.00745126 (lr:0.0001505248864545312)\n",
      "8200: training accuracy:1.0 training loss: 0.0427157 (lr:0.00014806075866510764)\n",
      "8300: training accuracy:1.0 training loss: 0.00927387 (lr:0.00014571680780607802)\n",
      "8400: training accuracy:1.0 training loss: 0.0326348 (lr:0.00014348717277938534)\n",
      "8500: training accuracy:1.0 training loss: 0.00985161 (lr:0.00014136627833609785)\n",
      "8500: ********* test accuracy:1.0 test loss: 0.0227796\n",
      "8600: training accuracy:1.0 training loss: 0.00558738 (lr:0.00013934882113538272)\n",
      "8700: training accuracy:1.0 training loss: 0.0242273 (lr:0.00013742975648339164)\n",
      "8800: training accuracy:1.0 training loss: 0.103918 (lr:0.00013560428571889846)\n",
      "8900: training accuracy:1.0 training loss: 0.00474054 (lr:0.0001338678442141468)\n",
      "9000: training accuracy:1.0 training loss: 0.00209124 (lr:0.0001322160899609027)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000: ********* test accuracy:0.9998 test loss: 0.05779\n",
      "9100: training accuracy:1.0 training loss: 0.0225541 (lr:0.00013064489271317272)\n",
      "9200: training accuracy:1.0 training loss: 0.00192022 (lr:0.0001291503236594374)\n",
      "9300: training accuracy:1.0 training loss: 0.00102822 (lr:0.00012772864559857617)\n",
      "9400: training accuracy:1.0 training loss: 0.00633252 (lr:0.00012637630359491788)\n",
      "9500: training accuracy:1.0 training loss: 0.000518583 (lr:0.00012508991608904985)\n",
      "9500: ********* test accuracy:1.0 test loss: 0.00984504\n",
      "9600: training accuracy:1.0 training loss: 0.0886038 (lr:0.0001238662664421581)\n",
      "9700: training accuracy:1.0 training loss: 0.00928987 (lr:0.00012270229489275475)\n",
      "9800: training accuracy:1.0 training loss: 0.00209362 (lr:0.00012159509090568058)\n",
      "9900: training accuracy:1.0 training loss: 0.000900651 (lr:0.00012054188589425116)\n",
      "10000: training accuracy:1.0 training loss: 0.0244367 (lr:0.00011954004629734786)\n",
      "10000: ********* test accuracy:1.0 test loss: 0.0127942\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "def getBatch(i, size, trainFeatures, trainLabels):\n",
    "    startIndex = (i * size) % 42000\n",
    "    endIndex = startIndex + size\n",
    "    batch_X = trainFeatures[startIndex : endIndex]\n",
    "    batch_Y = trainLabels[startIndex : endIndex]\n",
    "    return batch_X, batch_Y\n",
    "\n",
    "# You can call this function in a loop to train the model, 100 images at a time\n",
    "def training_step(i):\n",
    "\n",
    "    # training on batches of 100 images with 100 labels\n",
    "    size = 100\n",
    "    batch_X, batch_Y = getBatch(i, size, trainFeatureVectorConvoFormat, ohtrainLabelNdarray)\n",
    "\n",
    "    # learning rate decay\n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)\n",
    "\n",
    "    # compute training values\n",
    "    if i % 100 == 0:\n",
    "        '''\n",
    "        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on batch_X and batch_Y (ie. on 100 pieces of data)\n",
    "        '''\n",
    "        a, c = sess.run([accuracy, cross_entropy], {X: batch_X, Y_: batch_Y, pkeep: 1.0})\n",
    "        print(str(i) + \": training accuracy:\" + str(a) + \" training loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "\n",
    "    # compute test values\n",
    "    if i % 500 == 0:\n",
    "        '''\n",
    "        When we sess.run here, we are calculating the accuracy and cross_entropy of the model on all of the data\n",
    "        '''\n",
    "        a, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorConvoFormat[-10000:], Y_: ohtrainLabelNdarray[-10000:], pkeep: 1.0})\n",
    "        print(str(i) + \": ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, pkeep: 0.75})\n",
    "\n",
    "# Run number of iterations training the NN    \n",
    "for i in range(10000+1): \n",
    "    training_step(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ********* test accuracy:1.0 test loss: 0.0128906\n"
     ]
    }
   ],
   "source": [
    "# Print the test accurscy on some data that was held out\n",
    "a, c = sess.run([accuracy, cross_entropy], {X: trainFeatureVectorConvoFormat[-10000:], Y_: ohtrainLabelNdarray[-10000:], pkeep: 1.0})\n",
    "print(\"\\n ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "\n",
    "# Get predictions on test data\n",
    "p = sess.run([predictions], {X: testFeatureVectorConvoFormat, pkeep: 1.0})\n",
    "\n",
    "# Write predictions to csv file\n",
    "results = pd.DataFrame({'ImageId': pd.Series(range(1, len(p[0]) + 1)), 'Label': pd.Series(p[0])})\n",
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
